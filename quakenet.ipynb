{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":99034189,"sourceType":"kernelVersion"},{"sourceId":99040579,"sourceType":"kernelVersion"},{"sourceId":103233430,"sourceType":"kernelVersion"},{"sourceId":220306377,"sourceType":"kernelVersion"},{"sourceId":223878581,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:05:46.77433Z","iopub.execute_input":"2025-02-06T11:05:46.77464Z","iopub.status.idle":"2025-02-06T11:05:46.7792Z","shell.execute_reply.started":"2025-02-06T11:05:46.774609Z","shell.execute_reply":"2025-02-06T11:05:46.778175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport json\n\nimport dill\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks, regularizers, ops\n\nimport zucaml as ml\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error, mean_squared_log_error, mean_absolute_percentage_error\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npd.set_option('display.max_columns', None)","metadata":{"papermill":{"duration":10.351293,"end_time":"2022-06-16T19:22:27.813638","exception":false,"start_time":"2022-06-16T19:22:17.462345","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:05:46.780146Z","iopub.execute_input":"2025-02-06T11:05:46.7805Z","iopub.status.idle":"2025-02-06T11:06:00.783582Z","shell.execute_reply.started":"2025-02-06T11:05:46.780478Z","shell.execute_reply":"2025-02-06T11:06:00.78279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class QuakeLoss(tf.keras.losses.Loss):\n    def __init__(self, number_points, scale_values, name=\"quake_loss\"):\n        super().__init__(name=name)\n\n        self.number_points = number_points\n        self.mag_scaler = scale_values['mag'][1]\n        self.coord_scaler = tf.constant([\n            scale_values['x'][1],\n            scale_values['y'][1],\n            scale_values['z'][1],\n        ], dtype=tf.float32)\n\n    def call(self, y_true, y_pred):\n\n        stacked_coord_scaler = tf.tile(\n            tf.expand_dims(self.coord_scaler, axis=0),\n            [tf.shape(y_true)[0], 1]\n        )\n\n        coords = {}\n        mags = {}\n        for flat_name, flat_y in {'y_true': y_true, 'y_pred': y_pred}.items():\n            reshaped_y = tf.reshape(flat_y, (-1, self.number_points, 4))\n            coords[flat_name] = reshaped_y[..., :3]\n            mags[flat_name] = reshaped_y[..., 3]\n\n        coords['y_true'], mags['y_true'] = tf.map_fn(\n            lambda inputs: self.reorder_closest_points(inputs[0], inputs[1], inputs[2], inputs[3]),\n            (coords['y_pred'], coords['y_true'], mags['y_true'], stacked_coord_scaler),\n            fn_output_signature=(tf.float32, tf.float32)\n        )\n\n        loss = tf.square((coords['y_pred'] - coords['y_true']) / self.coord_scaler)\n\n        loss = tf.reduce_mean(loss, axis=-1)\n\n        loss += tf.square((mags['y_pred'] - mags['y_true']) / self.mag_scaler)\n\n        loss = tf.reduce_mean(loss)\n\n        return loss\n\n    def reorder_closest_points(self, A, B, C, array_scaler):\n\n        sorted_indices = []\n        available_mask = tf.ones(tf.shape(B)[:1], dtype=tf.bool)\n    \n        A_scaled = A / array_scaler\n        B_scaled = B / array_scaler\n    \n        for i in range(A.shape[0]):\n    \n            distances = tf.reduce_sum(tf.square(A_scaled[i] - B_scaled), axis=1)\n    \n            distances = tf.where(available_mask, distances, tf.constant(float('inf'), dtype=tf.float32))\n    \n            closest_index = tf.argmin(distances)\n    \n            sorted_indices.append(closest_index)\n    \n            available_mask = tf.tensor_scatter_nd_update(available_mask, [[closest_index]], [False])\n    \n        return tf.gather(B, tf.stack(sorted_indices)), tf.gather(C, tf.stack(sorted_indices))\n\nclass QuakeActivation(tf.keras.layers.Layer):\n    def __init__(self, number_points, grid_values, **kwargs):\n        super().__init__(**kwargs)\n\n        self.number_points = number_points\n        self.grid_values = grid_values\n        self.sharpness = tf.Variable(-1, trainable=False, dtype=tf.float32)\n        self.activation_coord = self.rounded_vals\n        self.activation_mag = self.final_mag\n\n    def call(self, inputs):\n\n        reshaped = tf.reshape(inputs, (-1, self.number_points, 4))\n        coords = reshaped[..., :3]\n        mags = reshaped[..., 3]\n\n        coord_acts = []\n        for i, (coord_name, coord_vals) in enumerate(self.grid_values.items()): # python >= 3.7\n\n            coord_acts.append(\n                self.activation_coord(\n                    coords[..., i],\n                    coord_vals['min'],\n                    coord_vals['res'],\n                    self.sharpness,\n                )\n            )\n\n        mags = self.activation_mag(mags)\n\n        outputs = tf.stack([i for i in coord_acts] + [mags], axis=-1)\n        outputs = tf.reshape(outputs, (-1, self.number_points * 4))\n\n        return outputs\n\n    def identity_coord(self, x, v_min, v_step, sharpness): # same signature\n        return x\n\n    def smooth_round(self, x, sharpness):\n        return tf.sigmoid(sharpness * (x - tf.floor(x) - 0.5)) + tf.floor(x)\n\n    def discrete_vals(self, x, v_min, v_step, sharpness):\n        x = x - tf.constant(v_min, dtype=tf.float32)\n        x = x / tf.constant(v_step, dtype=tf.float32)\n        x = self.smooth_round(x, sharpness)\n        x = x * tf.constant(v_step, dtype=tf.float32)\n        x = x + tf.constant(v_min, dtype=tf.float32)\n        return x\n\n    def rounded_vals(self, x, v_min, v_step, sharpness): # same signature\n        x = x - tf.constant(v_min, dtype=tf.float32)\n        x = x / tf.constant(v_step, dtype=tf.float32)\n        x = tf.round(x)\n        x = x * tf.constant(v_step, dtype=tf.float32)\n        x = x + tf.constant(v_min, dtype=tf.float32)\n        x = tf.cast(tf.cast(x, dtype=tf.int32), dtype=tf.float32)\n        return x\n\n    def identity_mag(self, x): # same signature\n        return x\n\n    def relu_mag(self, x):\n        return tf.where(x <= 0.0, 0.0, x)\n\n    def final_mag(self, x):\n        return tf.where(x <= 0.0, 0.0, x)\n\nclass DiscreteScheduler(keras.callbacks.Callback):\n    def __init__(self, layer, scheduled_values):\n        super().__init__()\n        self.layer = layer\n        self.sharp_values = scheduled_values\n\n    def on_epoch_begin(self, epoch, logs=None):\n\n        current_sharp = float(self.layer.sharpness)\n\n        scheduled_sharp = self.get_from_schedule(epoch, current_sharp, self.sharp_values)\n\n        ################################\n        self.layer.activation_coord = self.layer.relu_mag\n        ################################\n\n        if scheduled_sharp < 0:\n            self.layer.activation_coord = self.layer.identity_coord\n            self.layer.sharpness.assign(scheduled_sharp)\n        else:\n            self.layer.activation_coord = self.layer.discrete_vals\n            self.layer.sharpness.assign(scheduled_sharp)\n\n    def get_from_schedule(self, epoch, default_or_previous_value, schedule):\n        if epoch < schedule[0][0] or epoch > schedule[-1][0]:\n            return default_or_previous_value\n        for i in range(len(schedule)):\n            if epoch == schedule[i][0]:\n                return schedule[i][1]\n        return default_or_previous_value\n\n    def on_train_end(self, logs=None):\n        self.layer.activation_coord = self.layer.rounded_vals\n        self.layer.activation_mag = self.layer.final_mag","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T11:06:00.784714Z","iopub.execute_input":"2025-02-06T11:06:00.785396Z","iopub.status.idle":"2025-02-06T11:06:00.808491Z","shell.execute_reply.started":"2025-02-06T11:06:00.785359Z","shell.execute_reply":"2025-02-06T11:06:00.807496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### gold","metadata":{"papermill":{"duration":0.007349,"end_time":"2022-06-16T19:22:27.828648","exception":false,"start_time":"2022-06-16T19:22:27.821299","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_gold = pd.read_csv('data/gold.csv')\n\nsort_order = ['date', 'x', 'y', 'z']\n\ndf_gold = df_gold.sort_values(sort_order, ascending=True).reset_index(drop=True)\n\nml.print_memory(df_gold)\ndf_gold[:5]","metadata":{"papermill":{"duration":1.488395,"end_time":"2022-06-16T19:22:29.324843","exception":false,"start_time":"2022-06-16T19:22:27.836448","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-02-06T11:06:00.810723Z","iopub.execute_input":"2025-02-06T11:06:00.811101Z","iopub.status.idle":"2025-02-06T11:07:54.366892Z","shell.execute_reply.started":"2025-02-06T11:06:00.811067Z","shell.execute_reply":"2025-02-06T11:07:54.365924Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### features","metadata":{"papermill":{"duration":0.007559,"end_time":"2022-06-16T19:22:29.340342","exception":false,"start_time":"2022-06-16T19:22:29.332783","status":"completed"},"tags":[]}},{"cell_type":"code","source":"discarded_features = [\n    'energy',\n]\n\ntarget = 'target'\ntime_ref = 'date'\npid = 'zone_frame'\nx_dim = 'x'\ny_dim = 'y'\nz_dim = 'z'\n\nlocation_features = [x_dim, y_dim, z_dim]\n\nunique_locations = {}\nfor dim in location_features:\n    unique_locations[dim] = list(df_gold[dim].unique())\n\ngrid_info = {}\nfor i in location_features:\n    grid_info[i] = {}\n    all_res = set(np.diff(np.sort(df_gold[i].unique())))\n    assert(len(all_res) == 1)\n    grid_info[i]['res'] = int(min(all_res))\n    grid_info[i]['min'] = int(df_gold[i].min())\n    grid_info[i]['max'] = int(df_gold[i].max())\n\ntime_info = {}\ntime_res = set(np.diff(np.sort(df_gold[time_ref].unique())) / np.timedelta64(1, 'D'))\nassert(len(time_res) == 1)\ntime_info['res'] = int(min(time_res))\n\ndiscarded_features += [target, time_ref, pid] + location_features\n\nremaining_features = [feature for feature in df_gold if feature not in discarded_features]\nall_features = [feature for feature in remaining_features if 'energy|rolling.mean' in feature]\nall_features += [feature for feature in remaining_features if 'energy|shift' in feature]\n\nleftover_features = [feature for feature in remaining_features if feature not in all_features]\nassert len(leftover_features) == 0, f'Leftover features'\n\nprint(f'Total features\\t\\t {len(all_features)}')","metadata":{"papermill":{"duration":0.021434,"end_time":"2022-06-16T19:22:29.369544","exception":false,"start_time":"2022-06-16T19:22:29.34811","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-02-04T21:33:10.541084Z","iopub.execute_input":"2025-02-04T21:33:10.54144Z","iopub.status.idle":"2025-02-04T21:33:11.065485Z","shell.execute_reply.started":"2025-02-04T21:33:10.541409Z","shell.execute_reply":"2025-02-04T21:33:11.064554Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_quakes = df_gold[df_gold[target] > 0].groupby(time_ref).agg({pid: 'count'})[pid].max()\n\nn_quakes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:33:11.066154Z","iopub.execute_input":"2025-02-04T21:33:11.066393Z","iopub.status.idle":"2025-02-04T21:33:11.187095Z","shell.execute_reply.started":"2025-02-04T21:33:11.066371Z","shell.execute_reply":"2025-02-04T21:33:11.186233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ground_zero_calc = QuakeActivation(n_quakes, grid_info)\n\nground_zero = {\n    x_dim: ground_zero_calc.rounded_vals(0, grid_info[x_dim]['min'], grid_info[x_dim]['res'], None).numpy(),\n    y_dim: ground_zero_calc.rounded_vals(0, grid_info[y_dim]['min'], grid_info[y_dim]['res'], None).numpy(),\n    z_dim: ground_zero_calc.rounded_vals(0, grid_info[z_dim]['min'], grid_info[z_dim]['res'], None).numpy(),\n}\n\nuse_cross_val = False\n\nscale_mag = True\n\nadd_timewise_section = False\ntimewise_number_layers = 4\ntimewise_number_filters = 2\n\nspatialtime_number_layers = 32\nspatialtime_number_filters = 16\n\nfullyconnected_config = [2**10] * 2\n\nbatch_size = 8\nepochs = 100\n\nuse_discrete_scheduler = True\nSHARPNESS_SCHEDULE = [\n    (0, -1e1),\n]\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta = 1e-3,\n    patience = 10,\n    restore_best_weights = True,\n)\n\noptimizer = keras.optimizers.Adam(\n    learning_rate = 1e-5,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:33:11.187895Z","iopub.execute_input":"2025-02-04T21:33:11.188141Z","iopub.status.idle":"2025-02-04T21:33:11.28315Z","shell.execute_reply.started":"2025-02-04T21:33:11.188119Z","shell.execute_reply":"2025-02-04T21:33:11.282129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"number_unique_pid = df_gold[pid].nunique()\nnumber_x_dim = df_gold[x_dim].nunique()\nnumber_y_dim = df_gold[y_dim].nunique()\nnumber_z_dim = df_gold[z_dim].nunique()\nnumber_features = len(all_features)\nnumber_timeframes = df_gold[time_ref].nunique()\n\n# make sure the full dataset is 'squared and full'\nassert(number_unique_pid == number_x_dim * number_y_dim * number_z_dim)\nassert(len(df_gold) == number_timeframes * number_unique_pid)\nassert(df_gold.groupby(pid).agg({time_ref: 'count'})[time_ref].nunique() == 1)\nassert(df_gold.groupby([pid, time_ref]).agg({target: 'count'})[target].nunique() == 1)\n\nprint(f'Unique')\nprint(f'PID: \\t\\t{number_unique_pid:,d}')\nprint(f'X: \\t\\t{number_x_dim}')\nprint(f'Y: \\t\\t{number_y_dim}')\nprint(f'Z: \\t\\t{number_z_dim}')\nprint(f'Features: \\t{number_features}')\nprint(f'Timeframe: \\t{number_timeframes:,d}')","metadata":{"execution":{"iopub.status.busy":"2025-02-04T21:33:11.284053Z","iopub.execute_input":"2025-02-04T21:33:11.284313Z","iopub.status.idle":"2025-02-04T21:33:21.765755Z","shell.execute_reply.started":"2025-02-04T21:33:11.28429Z","shell.execute_reply":"2025-02-04T21:33:21.764862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Split","metadata":{"papermill":{"duration":0.007564,"end_time":"2022-06-16T19:22:29.411166","exception":false,"start_time":"2022-06-16T19:22:29.403602","status":"completed"},"tags":[]}},{"cell_type":"code","source":"this_problem = ml.problems.MULTICLASS\n\ndf_train_val, df_test = ml.split_by_time_ref(df_gold, 0.9, target, time_ref, this_problem, True)\n\ndf_gold = []\n\nif not use_cross_val:\n    df_train, df_val = ml.split_by_time_ref(df_train_val, 1.0 - 0.1 / 0.9, target, time_ref, this_problem, True)\nelse:\n    df_train = df_train_val\n\ndf_train_val = []","metadata":{"papermill":{"duration":0.116198,"end_time":"2022-06-16T19:22:29.535364","exception":false,"start_time":"2022-06-16T19:22:29.419166","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-02-04T21:33:21.766614Z","iopub.execute_input":"2025-02-04T21:33:21.766879Z","iopub.status.idle":"2025-02-04T21:33:29.880053Z","shell.execute_reply.started":"2025-02-04T21:33:21.766857Z","shell.execute_reply":"2025-02-04T21:33:29.878999Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_scaling_values(coord_and_mag):\n\n    reshaped = np.reshape(coord_and_mag, (-1, n_quakes, 4))\n\n    values_scaling = {}\n    for i, coord in enumerate(location_features):\n        coord_values = reshaped[..., i]\n        values_scaling[coord] = (np.mean(coord_values), np.std(coord_values))\n\n    if scale_mag:\n        mag_values = reshaped[..., 3]\n        values_scaling['mag'] = (np.mean(mag_values), np.std(mag_values))\n    else:\n        values_scaling['mag'] = 0.0, 1.0\n\n    return values_scaling\n\ndef pad_group(group):\n\n    padded = group.to_dict(orient='records')\n\n    trailing = {\n        x_dim: ground_zero[x_dim],\n        y_dim: ground_zero[y_dim],\n        z_dim: ground_zero[z_dim],\n        target: 0\n    }\n\n    padded.extend([trailing] * (n_quakes - len(group)))\n\n    return padded\n\ndef transform_df(df, features, n_x_dim, n_y_dim, n_z_dim, n_features, preprocess, xyz_scaler):\n\n    transformed = df[features].copy()\n    transformed[time_ref] = df[time_ref]\n    transformed[pid] = df[pid]\n    transformed[target] = df[target]\n    transformed[x_dim] = df[x_dim]\n    transformed[y_dim] = df[y_dim]\n    transformed[z_dim] = df[z_dim]\n\n    return pivot_df(transformed, features, n_x_dim, n_y_dim, n_z_dim, n_features, xyz_scaler)\n\ndef pivot_df(df, features, n_x_dim, n_y_dim, n_z_dim, n_features, xyz_scaler):\n\n    # make sure order is correct and save unique_time_ref for later check\n    df = df.sort_values(sort_order, ascending=True).reset_index(drop=True)\n    unique_time_ref =  df[time_ref].unique()\n\n    # make sure every pid has the same count of time_ref (rows, and then unique)\n    assert(df.groupby(pid).agg({time_ref: 'count'})[time_ref].nunique() == 1)\n    assert(df.groupby([pid, time_ref]).agg({target: 'count'})[target].nunique() == 1)\n\n    n_unique_pid = df[pid].nunique()\n    # make sure that unique zones IN THIS dataframe are the same as found in global\n    # there is a previous assert that check global_unique_pid == n_x * n_y * n_z\n    assert(n_unique_pid == n_x_dim * n_y_dim * n_z_dim)\n\n    n_unique_time_ref = df[time_ref].nunique()\n    # make sure that unique time frames IN THIS dataframe are correct\n    assert(len(df) == n_unique_time_ref * n_unique_pid)\n\n    # features\n    np_features = np.asarray(df[features])\n    np_features = np.reshape(np_features, (n_unique_time_ref, n_x_dim, n_y_dim, n_z_dim, n_features))\n\n    # rows in features_pivot == rows in original / number unique pid [THIS CHECK IS DUPLICATED by check timeref same as global]\n    assert(np_features.shape[0] == n_unique_time_ref == df.shape[0] / n_unique_pid)\n\n    # check number of NaN is maitained\n    assert(pd.DataFrame(np_features.reshape(-1, n_features)).isna().sum().sum() == df.isna().sum().sum())\n\n    # target\n    np_target = df.loc[:, [time_ref, x_dim, y_dim, z_dim, target]].copy()\n\n    original_time_refs = list(np_target[time_ref].unique())\n\n    np_target = np_target[np_target[target] > 0]\n\n    result_time_refs = list(np_target[time_ref].unique())\n\n    no_events = pd.DataFrame([\n        {\n            time_ref: i,\n            x_dim: ground_zero[x_dim],\n            y_dim: ground_zero[y_dim],\n            z_dim: ground_zero[z_dim],\n            target: 0,\n        }\n        for i in original_time_refs\n        if i not in result_time_refs\n    ])\n\n    np_target = pd.concat([np_target, no_events])\n    np_target = np_target.sort_values(sort_order, ascending=True)\n    np_target = np_target.reset_index(drop=True)\n\n    np_target = np_target.groupby(time_ref)[location_features + [target]].apply(pad_group)\n    # check if they have the same (order and length) time_ref # this can fail if df_target is not padded time-wise\n    assert(np.all(np_target.index == unique_time_ref))\n    np_target = pd.DataFrame(np_target.to_list())\n    np_target = pd.concat([pd.json_normalize(np_target[col]) for col in np_target], axis=1, ignore_index=True)\n    np_target = np.asarray(np_target)\n\n    # normalize location and mag\n    if xyz_scaler is None:\n        xyz_scaler = get_scaling_values(np_target)\n\n    # check if resulting target shape is as expected: rows - one for each date, cols - possible quakes * (3D + mag)\n    assert(np_target.shape == (n_unique_time_ref, n_quakes * 4))\n\n    return np_features, np_target, xyz_scaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:33:29.881061Z","iopub.execute_input":"2025-02-04T21:33:29.881345Z","iopub.status.idle":"2025-02-04T21:33:29.896107Z","shell.execute_reply.started":"2025-02-04T21:33:29.88132Z","shell.execute_reply":"2025-02-04T21:33:29.895089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_X, train_y, values_scaler = transform_df(\n    df_train, all_features, number_x_dim, number_y_dim, number_z_dim, number_features, None, None\n)\n\nif not use_cross_val:\n    val_X, val_y, _ = transform_df(\n        df_val, all_features, number_x_dim, number_y_dim, number_z_dim, number_features, None, values_scaler\n    )\n\ntest_X, test_y, _ = transform_df(\n    df_test, all_features, number_x_dim, number_y_dim, number_z_dim, number_features, None, values_scaler\n)\n\n# make sure [first] all have the same shape and [second] the timeframe summed is equal to original\nfor dim in [1, 2, 3]:\n    assert(train_X.shape[dim] == test_X.shape[dim])\n    if not use_cross_val:\n        assert(train_X.shape[dim] == val_X.shape[dim])\ntimeframe_summed = train_X.shape[0] + test_X.shape[0]\nif not use_cross_val:\n    timeframe_summed += val_X.shape[0]\nassert(number_timeframes == timeframe_summed)\n\n# make sure [first] all have the same shape and [second] the timeframe summed is equal to original\nfor dim in [1]:\n    assert(train_y.shape[dim] == test_y.shape[dim])\n    if not use_cross_val:\n        assert(train_y.shape[dim] == val_y.shape[dim])\ntimeframe_summed = train_y.shape[0] + test_y.shape[0]\nif not use_cross_val:\n    timeframe_summed += val_y.shape[0]\nassert(number_timeframes == timeframe_summed)\n\ntrain_X.shape, values_scaler","metadata":{"execution":{"iopub.status.busy":"2025-02-04T21:33:29.897028Z","iopub.execute_input":"2025-02-04T21:33:29.897279Z","iopub.status.idle":"2025-02-04T21:34:05.319699Z","shell.execute_reply.started":"2025-02-04T21:33:29.897257Z","shell.execute_reply":"2025-02-04T21:34:05.31846Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### NN","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"def add_conv1d_block(number_filters, shape_kernel, l2_value, dropout_value, name_prefix):\n    def _block(input_tensor):\n        fx = layers.DepthwiseConv1D(\n            kernel_size=shape_kernel,\n            padding='valid',\n            depth_multiplier=number_filters,\n            depthwise_regularizer=regularizers.L2(l2_value),\n            activation='relu',\n            name=f'{name_prefix}_conv1d',\n        )(input_tensor)\n        fx = keras.layers.Dropout(dropout_value, name=f'{name_prefix}_dropout')(fx)\n        fx = layers.BatchNormalization(name=f'{name_prefix}_batchnorm')(fx)\n        return fx\n    return _block\n\ndef add_conv3d_block(number_filters, shape_kernel, l2_value, dropout_value, name_prefix):\n    def _block(input_tensor):\n        fx = layers.Conv3D(\n            filters=number_filters,\n            kernel_size=shape_kernel,\n            padding='same',\n            kernel_regularizer=regularizers.L2(l2_value),\n            activation='relu',\n            name=f'{name_prefix}_conv3d',\n        )(input_tensor)\n        fx = keras.layers.Dropout(dropout_value, name=f'{name_prefix}_dropout')(fx)\n        fx = layers.BatchNormalization(name=f'{name_prefix}_batchnorm')(fx)\n        return fx\n    return _block","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:34:05.320749Z","iopub.execute_input":"2025-02-04T21:34:05.321075Z","iopub.status.idle":"2025-02-04T21:34:05.327798Z","shell.execute_reply.started":"2025-02-04T21:34:05.321041Z","shell.execute_reply":"2025-02-04T21:34:05.326788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#####################################\n# Input\n#####################################\ninputs = keras.Input(shape=(number_x_dim, number_y_dim, number_z_dim, number_features), name='input')\nscale_layer = keras.layers.Normalization(axis=-1)\ninput_scaled = scale_layer(inputs)\n\n#####################################\n# Time-wise section\n#####################################\nfirst_section = input_scaled\n\nif add_timewise_section:\n    section_name = '1st_section_'\n\n    first_section = layers.Reshape(\n        (-1, number_features),\n        name=f'{section_name}convert_to_1D',\n    )(first_section)\n    first_section = layers.Permute((2, 1), name=f'{section_name}zone_as_features')(first_section)\n\n    for i in range(timewise_number_layers):\n        new_block = add_conv1d_block(\n            number_filters=timewise_number_filters if i == 0 else 1,\n            shape_kernel=3,\n            l2_value=1e-2,\n            dropout_value=0.2,\n            name_prefix=f'{section_name}{i + 1}',\n        )\n        first_section = new_block(first_section)\n\n    first_section = layers.Permute((2, 1), name=f'{section_name}zone_as_spatial')(first_section)\n    first_section = layers.Reshape(\n        (number_x_dim, number_y_dim, number_z_dim, -1),\n        name=f'{section_name}convert_back_to_3D',\n    )(first_section)\n\n#####################################\n# Spatial-time section\n#####################################\nsecond_section = first_section\n\nfor i in range(spatialtime_number_layers):\n    new_block = add_conv3d_block(\n        number_filters=spatialtime_number_filters,\n        shape_kernel=3,\n        l2_value=1e-2,\n        dropout_value=0.2,\n        name_prefix=f'2nd_section_{i + 1}',\n    )\n    second_section = new_block(second_section)\n\n#####################################\n# FC section\n#####################################\nfc_section = layers.Flatten(name='fc_section_flat')(second_section)\n\nfor i, number_neurons in enumerate(fullyconnected_config):\n    fc_section = layers.Dense(number_neurons, activation='relu', name=f'fc_section_{i + 1}')(fc_section)\n\n#####################################\n# Output section\n#####################################\noutputs = layers.Dense(n_quakes * 4, activation=None)(fc_section)\nfinal_activation_layer = QuakeActivation(n_quakes, grid_info)\noutputs = final_activation_layer(outputs)\n\n#####################################\n# Create\n#####################################\nmodel = keras.Model(inputs=inputs, outputs=outputs, name='quake_net')\n\n#####################################\n# Compile\n#####################################\nmodel.compile(\n    optimizer = optimizer,\n    loss = QuakeLoss(n_quakes, values_scaler),\n)\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:34:05.328843Z","iopub.execute_input":"2025-02-04T21:34:05.329177Z","iopub.status.idle":"2025-02-04T21:34:05.669212Z","shell.execute_reply.started":"2025-02-04T21:34:05.329144Z","shell.execute_reply":"2025-02-04T21:34:05.668423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"%%time\n\ncallbacks_to_use = [early_stopping]\nif use_discrete_scheduler:\n    callbacks_to_use.append(DiscreteScheduler(final_activation_layer, SHARPNESS_SCHEDULE))\n\nfit_params = {\n    'x': train_X,\n    'y': train_y,\n    'batch_size': batch_size,\n    'epochs': epochs,\n    'callbacks': callbacks_to_use,\n}\n\nif use_cross_val:\n    fit_params['validation_split'] = 0.1\nelse:\n    fit_params['validation_data'] = (val_X, val_y)\n\nhistory = model.fit(**fit_params)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:34:05.670104Z","iopub.execute_input":"2025-02-04T21:34:05.670358Z","iopub.status.idle":"2025-02-04T21:41:29.528832Z","shell.execute_reply.started":"2025-02-04T21:34:05.670334Z","shell.execute_reply":"2025-02-04T21:41:29.527798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\n\nhistory_df[history_df['val_loss'] == history_df['val_loss'].min()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:41:29.530095Z","iopub.execute_input":"2025-02-04T21:41:29.530489Z","iopub.status.idle":"2025-02-04T21:41:29.935489Z","shell.execute_reply.started":"2025-02-04T21:41:29.530453Z","shell.execute_reply":"2025-02-04T21:41:29.934444Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Score model","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"sets = {\n    'train': (df_train, train_X, train_y),\n    'test': (df_test, test_X, test_y),\n}\n\nflats = {}\n\nfor set_name, (df, np_X, np_y) in sets.items():\n    \n    score_df = (\n        pd\n        .DataFrame(df[time_ref].unique(), columns=[time_ref])\n        .sort_values(time_ref, ascending=True)\n        .reset_index(drop=True)\n    )\n\n    y_pred = model.predict(np_X)\n    y_true = np_y.copy()\n\n    flats[set_name] = (y_pred, y_true, score_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:41:30.563754Z","iopub.execute_input":"2025-02-04T21:41:30.564033Z","iopub.status.idle":"2025-02-04T21:42:15.496335Z","shell.execute_reply.started":"2025-02-04T21:41:30.563999Z","shell.execute_reply":"2025-02-04T21:42:15.495431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def expand_pred(preds):\n\n    def expand_rows(row):\n        expanded_rows = []\n        for col in location_features:\n            for add_sub in [1, -1]:\n                new_row_add = row.copy()\n                new_row_add[col] += add_sub * grid_info[col]['res']\n                expanded_rows.append(new_row_add)\n        return expanded_rows\n    \n    all_rows = []\n\n    # TODO: improve this, although this is not long\n    for _, row in preds.iterrows():\n        all_rows.append(row)\n        all_rows.extend(expand_rows(row))\n\n    expanded = pd.DataFrame(all_rows, columns=preds.columns).reset_index(drop=True)\n    expanded[location_features] = expanded[location_features].astype(int)\n\n    return expanded\n\n# order is important\ngrand_metrics = {\n    1: 'tp',\n    2: 'total_true_positives',\n    3: 'total_pred_positives',\n}\ndef create_grand_metrics(df):\n    for grand_metric_k in sorted(grand_metrics):\n        df[grand_metrics[grand_metric_k]] = 0\n    return df\n\ndef insert_grand_metrics(df, i, metrics_values):\n    for metric_k, metric in grand_metrics.items():\n        df.iloc[i, metric_k] = metrics_values[metric]\n    return df\n\ndef reshape_flats(preds, trues):\n    coordinates = {}\n    magnitudes = {}\n    for flat_name, flat_y in {'y_true': trues, 'y_pred': preds}.items():\n\n        reshape_y = np.reshape(flat_y, (flat_y.shape[0], n_quakes, 4))\n        \n        coordinates[flat_name] = reshape_y[..., :3]\n        magnitudes[flat_name] = reshape_y[..., 3]\n\n    return coordinates, magnitudes\n\ndef calculate_tps(coords_pred, mags_pred, coords_true, mags_true, margin_tp, margin_fp, expand_preds):\n\n    res_metrics = {}\n\n    res_true = pd.DataFrame(coords_true)\n    res_true = pd.concat([res_true, pd.DataFrame(mags_true)], axis=1)\n    res_true.columns = location_features + ['mag_true']\n    res_true = res_true[res_true['mag_true'] > 0].reset_index(drop=True)\n    res_true[location_features] = res_true[location_features].astype(int)\n    res_metrics['total_true_positives'] = len(res_true)\n\n    res_pred = pd.DataFrame(coords_pred)\n    res_pred = pd.concat([res_pred, pd.DataFrame(mags_pred)], axis=1)\n    res_pred.columns = location_features + ['mag_pred']\n    res_pred = res_pred[res_pred['mag_pred'] > 0].reset_index(drop=True)\n    res_pred[location_features] = res_pred[location_features].astype(int)\n    res_metrics['total_pred_positives'] = len(res_pred[res_pred['mag_pred'] > margin_fp])\n    if expand_preds:\n        res_pred = expand_pred(res_pred)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        res_true = res_true.merge(\n            res_pred,\n            on=location_features,\n            how='left',\n        )\n\n    res_true['mag_abs_diff'] = (res_true['mag_pred'] - res_true['mag_true']).abs()\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        res_metrics['tp'] = (res_true['mag_abs_diff'] < margin_tp).sum()\n\n    return res_metrics\n\ndef calculate_scores(margin_tp, margin_fp, expand_predictions = False):\n\n    epsilon = np.finfo(float).eps\n\n    scores = {}\n    for set_name, (y_pred, y_true, score_df) in flats.items():\n\n        scores[set_name] = {}\n        coords, mags = reshape_flats(y_pred, y_true)\n        score_df = create_grand_metrics(score_df)\n\n        for i in range(y_pred.shape[0]):\n\n            metrics_values = calculate_tps(\n                coords_pred=coords['y_pred'][i],\n                mags_pred=mags['y_pred'][i],\n                coords_true=coords['y_true'][i],\n                mags_true=mags['y_true'][i],\n                margin_tp=margin_tp,\n                margin_fp=margin_fp,\n                expand_preds=expand_predictions,\n            )\n\n            score_df = insert_grand_metrics(score_df, i, metrics_values)\n\n        for i in grand_metrics.values():\n            scores[set_name][i] = score_df[i].sum()\n\n    for set_name in flats:\n        scores[set_name]['fn'] = scores[set_name]['total_true_positives'] - scores[set_name]['tp']\n        scores[set_name]['fp'] = scores[set_name]['total_pred_positives'] - scores[set_name]['tp']\n        precision = scores[set_name]['tp'] / (scores[set_name]['tp'] + scores[set_name]['fp'] + epsilon)\n        recall = scores[set_name]['tp'] / (scores[set_name]['tp'] + scores[set_name]['fn'] + epsilon)\n        scores[set_name]['precision'] = precision\n        scores[set_name]['recall'] = recall\n\n        scores[set_name]['f1_score'] = 2 * precision * recall / (precision + recall + epsilon)\n\n    scores['test']['overfit'] = scores['train']['f1_score'] - scores['test']['f1_score']\n\n    return scores\n\ncalculate_scores(1, 1)['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T21:42:15.502539Z","iopub.execute_input":"2025-02-04T21:42:15.502902Z","iopub.status.idle":"2025-02-04T21:44:21.944877Z","shell.execute_reply.started":"2025-02-04T21:42:15.502865Z","shell.execute_reply":"2025-02-04T21:44:21.9432Z"}},"outputs":[],"execution_count":null}]}