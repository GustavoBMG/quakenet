{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30197,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import copy\nimport json\nimport os\n\nimport pandas as pd\nimport numpy as np\n\n#############################################################################################################\n### global_vars.py\n#############################################################################################################\n\nfrom enum import Enum, IntEnum\n\nproblems = IntEnum('Problems', 'REGRESSION BINARY BINARY_AUTENC BINARY_CONV MULTICLASS') #threshold, multilabel zero-inflated\n\ncv_strategies = IntEnum('CV_Strategies', 'KFOLD TIME')\n\n#############################################################################################################\n### global_vars.py\n#############################################################################################################\n\nimport hashlib\nimport multiprocessing\n\nn_cores = multiprocessing.cpu_count()\n\ndef get_memory(df):\n    return df.memory_usage().sum() / (1024 * 1024)\n\ndef print_memory(df):\n    print('Memory usage: {0:.2f}'.format(get_memory(df)) + ' MB')\n    print('{0:,d} x {1:,d}'.format(df.shape[0], df.shape[1]))\n    \ndef print_memory_in(df):\n    print(f'Memory usage: {get_memory(df):.2f} MB\\t\\t Size: {df.shape[0]:,d} x {df.shape[1]:,d}\\n')\n\n#TODO: rename to get string form model.\n#If used for tracking, create one to get the json, but string all values (funcions cannot be serialize in json dump)\ndef get_string(config):\n    \n    hashed_string = ''\n    \n    for section in config:\n        hashed_string += '|' + section + '|'\n        if isinstance(config[section], dict):\n            for param in config[section]:\n                hashed_string += '|' + param + '_' + str(config[section][param]) + '|'\n        else:\n            hashed_string += '|' + str(config[section]) + '|'\n            \n    return hashed_string\n    \ndef hash_string(label, num_char = None):\n\n    label_encoded = label.encode('utf-8')\n    \n    hash_object = hashlib.md5(label_encoded)\n    \n    hash_string = hash_object.hexdigest()\n    \n    if not num_char is None:\n        hash_string = hash_string[:num_char]\n    \n    return hash_string\n\ndef get_random_string(num_char = 5):\n    \n    return hash_string(str(np.random.randint(0, 10000)), num_char)\n    \ndef distribute_dfs(df, partition):\n\n    items_count = df[partition].value_counts()\n\n    buckets = {}\n\n    for bucket in range(0, n_cores):\n        buckets[bucket] = []\n\n    count_order = 0\n    for k, v in sorted(items_count.items(), key = lambda item: item[1], reverse = True):\n\n        buckets[count_order % n_cores].append(k)\n\n        count_order += 1\n        \n    return buckets\n    \ndef distribute(df, item, func, params):\n\n    buckets = distribute_dfs(df = df, partition = item)\n\n    pool_params = []\n\n    for bucket in buckets:\n\n        df_sliced = df.loc[df[item].isin(buckets[bucket])].copy()\n\n        params_sliced = copy.deepcopy(params)\n\n        params_sliced['df'] = df_sliced\n\n        params_sliced = tuple(params_sliced.values())\n\n        pool_params.append(params_sliced)\n    \n    with multiprocessing.Pool(n_cores) as pool:\n        dfs = pool.starmap(func, pool_params)\n\n    return pd.concat(dfs)\n\n#############################################################################################################\n### etl.py\n#############################################################################################################\n\ndef reindex_by_minmax(df, item, time_ref, time_freq, forwardfill_features, backfill_features, zerofill_features):\n    return distribute(df = df, item = item, func = single_core_reindex_by_minmax, params = locals()).reset_index().drop('index', axis = 1)\n\ndef single_core_reindex_by_minmax(df, item, time_ref, time_freq, forwardfill_features, backfill_features, zerofill_features):\n\n    drop_feats = []\n\n    #### create reset\n    df = create_reset(\n        df = df,\n        item = item,\n        time_ref = time_ref,\n        order = None,\n    )\n\n    #### reindex\n    df = df.groupby(item).resample(time_freq, on=time_ref, closed='left', include_groups=False).sum()\n    df = df.reset_index()\n\n    #### ordering to fill by pair\n    df = create_reset(\n        df = df,\n        item = item,\n        time_ref = time_ref,\n        order = None,\n    )\n    drop_feats.append('reset')\n\n    #### ffill\n    for feat in forwardfill_features:\n        df[feat] = df.groupby(['reset'])[feat].fillna(method = 'ffill')\n\n    #### bfill\n    for feat in backfill_features:\n        df[feat] = df.groupby(['reset'])[feat].fillna(method = 'bfill')\n\n    #### fill 0 doesnt matter order\n    for feat in zerofill_features:\n        df[feat] = df[feat].fillna(0)\n\n    #### aux\n    df = df.drop(drop_feats, axis = 1)\n\n    return df\n\ndef pad(df, item, time_ref, all_items, boundry, zero_fill, other_fill):\n\n    if boundry == 'min':\n        boundry_time = df[time_ref].min()\n    elif boundry == 'max':\n        boundry_time = df[time_ref].max()\n    else:\n        print(f'\\n Unknown boundry: {str(boundry)} \\n')\n    \n    boundry_items = df[df[time_ref] == boundry_time][item].unique()\n    items_without_boundry = [i for i in all_items if i not in boundry_items]\n    \n    features_without_fill = []\n    dict_to_add = {}\n    for feature in df:\n        if feature == item:\n            dict_to_add[item] = items_without_boundry\n        elif feature == time_ref:\n            dict_to_add[time_ref] = boundry_time\n        else:\n            if feature in zero_fill:\n                dict_to_add[feature] = 0\n            elif feature in other_fill:\n                dict_to_add[feature] = other_fill[feature]\n            else:\n                dict_to_add[feature] = np.nan\n                features_without_fill.append(feature)\n\n    df_to_add = pd.DataFrame(dict_to_add)\n    \n    assert(df_to_add[[feature for feature in df_to_add if feature not in features_without_fill]].isna().sum().sum() == 0)\n    \n    df = pd.concat([df_to_add, df])\n    \n    df = df.sort_values([item, time_ref], ascending = [True, True]).reset_index().drop('index', axis = 1)\n    \n    return df\n\n#############################################################################################################\n### feature_engineering.py\n#############################################################################################################\n\ndef create_reset(df, item, time_ref, order):\n    \n    if order is None:\n        order = [True, True]\n    \n    df = df.sort_values([item, time_ref], ascending = order)\n\n    df['reset'] = (df[item].shift(1) != df[item]) * 1\n\n    df['reset'] = df['reset'].cumsum()\n    \n    return df\n\ndef get_feat_label(label, feature_1, feature_2, func, func_val):\n    \n    str_feature_2 = str(feature_2)\n    \n    if label is None:\n        if not feature_2 is None:\n            label = feature_1 + '||' + func + '||' + str_feature_2\n        else:\n            label = feature_1 + '|' + func\n            if not func_val is None:\n                label += '#' + str(func_val)\n\n    return label\n\ndef math_feature(df, feature_1, feature_2, func, label):\n    \n    label = get_feat_label(label, feature_1, feature_2, func, None)\n    \n    if func == 'ratio':\n        df[label] = df[feature_1] / df[feature_2]\n    elif func == 'diff':\n        df[label] = df[feature_1] - df[feature_2]\n    elif func == 'mult':\n        df[label] = df[feature_1] * df[feature_2]\n    elif func == 'add':\n        df[label] = df[feature_1] + df[feature_2]\n    elif func == 'diff.days':\n        df[label] = (df[feature_1] - df[feature_2]).dt.days\n    else:\n        print(f'Error - math function not found: {func}')\n        return\n\n    return df\n\ndef ts_feature(df, feature_base, func, func_val, label):\n    return distribute(df = df, item = 'reset', func = single_core_ts_feature, params = locals())\n\ndef single_core_ts_feature(df, feature_base, func, func_val, label):\n\n    label = get_feat_label(label, feature_base, None, func, func_val)\n\n    if func.startswith('rolling'):\n        df[label] = df.groupby(['reset'])[feature_base].rolling(window = func_val, min_periods = 1).agg(func.split('.')[1]).reset_index().set_index('level_1')[feature_base]\n    elif func.startswith('cum'):\n        df[label] = df.groupby(['reset'])[feature_base].agg(func)\n    elif func == 'shift':\n        df[label] = df.groupby(['reset'])[feature_base].shift(func_val)\n    elif func.endswith('fill'):\n        df[label] = df.groupby(['reset'])[feature_base].fillna(method = func)\n    elif func.startswith('count'):\n        _, count_operator, count_val = func.split('_')\n        \n        if '.' in count_val:\n            count_val = float(count_val)\n        else:\n            count_val = int(count_val)\n            \n        df = comparison_feature(\n            df = df,\n            feature_base = feature_base,\n            comp_val = count_val,\n            comp_operator = count_operator,\n            label = label,\n        )\n\n        df[label] = df.groupby(['reset'])[label].rolling(window = func_val, min_periods = 1).agg('sum').reset_index().set_index('level_1')[label]\n    else:\n        print(f'Error - ts function not found: {func}')\n        return\n\n    return df\n\ndef track_feature(df, feature_base, condition, track_window, track_function, label):\n    \n    #### features names\n    if label is None:\n        label = 'track.' + feature_base + '.cond.' + track_function + '#' + str(track_window)\n        if label in df:\n            label += mlutil.get_random_string()\n            \n    feature_condition = feature_base + '_cond_' + get_random_string(10)\n    feature_condition_shifted = feature_condition + '_shift_' + get_random_string(10)\n    feature_condition_shifted_filled = feature_condition_shifted + '_fill_' + get_random_string(10)\n    \n    #### drop features\n    drop_features = []\n    drop_features.append(feature_condition)\n    drop_features.append(feature_condition_shifted)\n    drop_features.append(feature_condition_shifted_filled)\n    \n    #### calculate the condition\n    df[feature_condition] = np.nan\n    df.loc[condition, feature_condition] = df.loc[condition, feature_base]\n    \n    #### shift in time\n    df = ts_feature(\n        df = df,\n        feature_base = feature_condition,\n        func = 'shift',\n        func_val = track_window,\n        label = feature_condition_shifted,\n    )\n\n    #### forward fill - important to leave the nan before so here it works\n    df = ts_feature(\n        df = df,\n        feature_base = feature_condition_shifted,\n        func = 'ffill',\n        func_val = None,\n        label = feature_condition_shifted_filled,\n    )\n    \n    #### TODO: group before to calculate in other granularities, like week etc\n    #### calculate the function\n    df = math_feature(\n        df = df,\n        feature_1 = feature_base,\n        feature_2 = feature_condition_shifted_filled,\n        func = track_function,\n        label = label,\n    )\n    \n    #### aux\n    df = df.drop(drop_features, axis = 1)\n    \n    return df\n\n#############################################################################################################\n### problem_statement.py\n#############################################################################################################\n\ndef set_target(df, item, time_ref, target, early_warning, range_warning, drop_na_target, target_name = None):\n\n    new_feat = 'target' if target_name is None else target_name\n    drop_feats = []\n\n    df = create_reset(df = df, item = item, time_ref = time_ref, order = None)\n    drop_feats.append('reset')\n    \n    if range_warning > 1:\n        df = ts_feature(\n            df = df,\n            feature_base = target,\n            func = 'rolling.sum',\n            func_val = range_warning,\n            label = new_feat,\n        )\n    else:\n        range_warning = 1\n        df[new_feat] = df[target]\n\n    df = ts_feature(\n        df = df,\n        feature_base = new_feat,\n        func = 'shift',\n        func_val = -(range_warning - 1 + early_warning),\n        label = new_feat,\n    )\n\n    na_feats = df.isna().sum()\n\n    na_feats = list(na_feats[na_feats > 0].index.values)\n\n    for feat in na_feats:\n        if feat != new_feat:\n            print(f'\\nWarning: Feature {feat} with na values\\n')\n\n    if df[new_feat].isna().sum() > df[item].nunique() * abs(range_warning - 1 + early_warning):\n        print(f'\\nWarning: Target with more na values\\n')\n        \n    if drop_na_target:\n        df = df.loc[~df[new_feat].isna()].copy()\n\n    df = df.sort_values([item, time_ref]).reset_index().drop('index', axis = 1)\n    \n    df = df.drop(drop_feats, axis = 1)\n\n    return df\n\n#############################################################################################################\n### file_handler.py\n#############################################################################################################\n\ndate_time_format = '%Y-%m-%d'   #  '%Y-%m-%d %H:%M:%S'\n\ndef get_csv(path, label, only_fields):\n    \n    with open(path + label + '_feats.txt') as json_file:\n        types_desc = json.load(json_file)\n\n    if len(only_fields) > 0:\n        to_discard = []\n        for feat in types_desc:\n            if feat not in only_fields:\n                print(f'Discarded:\\t{feat}')\n                to_discard.append(feat)\n        \n        for feat in to_discard:\n            types_desc.pop(feat)\n        \n        for feat in only_fields:\n            if feat not in types_desc:\n                print(f'Warning:\\t{feat} not found')\n                \n        only_fields = [feat for feat in only_fields if feat in types_desc]\n\n    dts = [feat for feat in types_desc if types_desc[feat].startswith('datetime')]\n    \n    for feat in dts:\n        types_desc.pop(feat)\n    \n    df = pd.read_csv(path + label + '.csv', dtype = types_desc, usecols = only_fields if len(only_fields) > 0 else None)\n    \n    if len(only_fields) > 0:\n        df = df[only_fields]\n\n    for dt in dts:\n        df[dt] = pd.to_datetime(df[dt], format = date_time_format, exact = True)\n\n    return df\n\ndef downcast_csv(df, unsigned_feats, integer_feats, float_feats, print_info):\n    \n    before = get_memory(df)\n\n    for feat in unsigned_feats:\n        if feat in df:\n            df[feat] = pd.to_numeric(df[feat], downcast = 'unsigned')\n        else:\n            print(f'Unsigned feature not found\\t{feat}')\n\n    for feat in integer_feats:\n        if feat in df:\n            df[feat] = pd.to_numeric(df[feat], downcast = 'integer')\n        else:\n            print(f'Integer feature not found\\t{feat}')\n\n    for feat in float_feats:\n        if feat in df:\n            df[feat] = pd.to_numeric(df[feat], downcast = 'float')\n        else:\n            print(f'Float feature not found  \\t{feat}')\n\n    after = get_memory(df)\n    \n    if print_info:\n        print('Downcast: {0:.0%}'.format(after / before - 1))\n    \n    return df\n    \ndef print_csv(df, path, label, print_index = False, print_info = True):\n\n    feat_dict = {}\n\n    for feat in df:\n        feat_dict[feat] = str(df[feat].dtype)\n\n    with open(path + label + '_feats.txt', 'w') as outfile:\n        json.dump(feat_dict, outfile)\n\n    df.to_csv(path + label + '.csv', index = print_index)\n    \n    if print_info:\n        print_memory_in(df)\n    \ndef print_results(df, path, label, model_config, feats_used, results, n_rows):\n    \n    file = str(label) + '__' + str(int(results['Score'] * 100)) + '.csv'\n    \n    df.to_csv(path + 'submit_csv/' + file, index = False)\n\n    general_stats = {}\n    general_stats['results'] = results\n    general_stats['n_feats'] = len(feats_used)\n    general_stats['feats'] = feats_used\n    general_stats['date'] = datetime.datetime.now().strftime('%H:%M %d/%m')\n    general_stats['n_rows'] = n_rows\n    general_stats['model'] = get_string(model_config)\n    \n    with open(path + label + '.txt', 'w') as outfile:\n        json.dump(general_stats, outfile)\n\n#############################################################################################################\n### split.py\n#############################################################################################################\n\ndef simple_slice_df(df, col, limit_floor, limit_ceil):\n    return df.loc[\n        (df[col] >= limit_floor) &\n        (df[col] < limit_ceil)\n    ].copy().reset_index(drop=True)\n\ndef split_by_time_ref(df, perc, target, time_ref, problem, print_details, df_target=None):\n\n    min_all = df[time_ref].min()\n    max_all = df[time_ref].max()\n\n    ##limit\n    if np.issubdtype(type(df.loc[0, time_ref]), np.integer):\n        train_floor = min_all + 0\n\n        train_ceil = int((max_all - train_floor) * perc) + train_floor\n\n        test_ceil = max_all + 1\n    else:\n        train_floor = min_all + pd.to_timedelta(0, unit = 'D')\n\n        train_ceil = (max_all - train_floor) * perc + train_floor\n\n        test_ceil = max_all + pd.to_timedelta(1, unit = 'D')\n\n    ##df\n    df_train = simple_slice_df(df, time_ref, train_floor, train_ceil)\n    df_test = simple_slice_df(df, time_ref, train_ceil, test_ceil)\n    assert(df_train[time_ref].max() < df_test[time_ref].min())\n\n    if df_target is not None:\n        df_target_train = simple_slice_df(df_target, time_ref, train_floor, train_ceil)\n        df_target_test = simple_slice_df(df_target, time_ref, train_ceil, test_ceil)\n        assert(df_target_train[time_ref].max() < df_target_test[time_ref].min())\n\n    if print_details:\n        print_balances([df_train, df_test], len(df), target, problem)\n\n    if df_target is not None:\n        return df_train, df_test, df_target_train, df_target_test\n    else:\n        return df_train, df_test\n\ndef print_balances(dfs, len_df, target, problem):\n    \n    if problem == problems.REGRESSION:\n        \n        header = ''\n        header += str.rjust('Total', 10) + '\\t'\n        header += str.rjust('Total perc', 10) + '\\t'\n        header += str.rjust('Median', 10) + '\\t'\n        header += str.rjust('Mean', 10) + '\\t'\n        header += str.rjust('Std', 10) + '\\t'\n        header += str.rjust('Min', 10) + '\\t'\n        header += str.rjust('Max', 10) + '\\t'\n        header += str.rjust('Zeros', 10) + '\\t'\n        print(header)\n\n        for df in dfs:\n\n            line = ''\n            line += f'{len(df):10,}\\t'\n            line += f'{len(df) / len_df: 10.0%}\\t'\n            line += f'{df[target].median(): 10.2f}\\t'\n            line += f'{df[target].mean(): 10.2f}\\t'\n            line += f'{df[target].std(): 10.2f}\\t'\n            line += f'{df[target].min(): 10.2f}\\t'\n            line += f'{df[target].max(): 10.2f}\\t'\n            line += f'{sum(df[target] == 0) / len(df): 10.0%}\\t'\n\n            print(line)\n        \n    elif problem == problems.BINARY:\n        \n        header = ''\n        header += str.rjust('Total', 10) + '\\t'\n        header += str.rjust('Total perc', 10) + '\\t'\n        header += str.rjust('Balance', 10) + '\\t'\n        header += str.rjust('Events', 10) + '\\t'\n        print(header)\n\n        for df in dfs:\n\n            line = ''\n            line += f'{len(df):10,}\\t'\n            line += f'{len(df) / len_df: 10.0%}\\t'\n            line += f'{df[target].sum() / len(df): 10.2%}\\t'\n            line += f'{df[target].sum(): 10,d}\\t'\n\n            print(line)\n\n    elif problem == problems.MULTICLASS:\n\n        all_classes = []\n\n        for df in dfs:\n            all_classes_df = (df[target].astype(int)).value_counts().to_dict()\n            all_classes += list(all_classes_df.keys())\n\n        all_classes = list(set(all_classes))\n        all_classes.sort()\n\n        header = ''\n        header += str.rjust('Total', 10) + '\\t'\n        header += str.rjust('Total perc', 10) + '\\t'\n        for ml_class in all_classes:\n            header += str.rjust(f'{ml_class}', 5) + '\\t'\n        print(header)\n\n        for df in dfs:\n\n            all_classes_df = (df[target].astype(int)).value_counts().to_dict()\n\n            line = ''\n            line += f'{len(df):10,}\\t'\n            line += f'{len(df) / len_df: 10.0%}\\t'\n            for ml_class in all_classes:\n                if ml_class in all_classes_df:\n                    this_class_balance = all_classes_df[ml_class] / len(df)\n                    if this_class_balance > 1e-2:\n                        line += f'{this_class_balance: 3.2%}\\t'\n                    elif this_class_balance > 1e-4:\n                        line += f'{this_class_balance * 1e4: 3.0f} bps\\t'\n                    else:\n                        line += f'{all_classes_df[ml_class]:>5,d} #\\t'\n                else:\n                    line += f'{0 / len(df): 3.2%}\\t'\n\n            print(line)\n\n    else:\n        print('\\n Unknown objective \\n')\n        return\n\n#############################################################################################################\n### \n#############################################################################################################","metadata":{"_uuid":"335073be-b25b-4ff3-90a6-00b43e597ba1","_cell_guid":"60d56463-3056-4553-b415-4a50b5092b53","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}